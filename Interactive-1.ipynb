{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import yaml\n",
    "import tqdm\n",
    "from PIL import Image\n",
    " \n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    " \n",
    "import wandb\n",
    " \n",
    " \n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image, label\n",
    " \n",
    "class Model(nn.Module):\n",
    "    \"\"\"Example of a model using PyTorch\n",
    " \n",
    "    This specific model is a pretrained ResNet18 model from torch hub\n",
    "    with a custom head.\n",
    " \n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    " \n",
    "        self.base = torch.hub.load(\n",
    "            \"pytorch/vision:v0.10.0\", \"mobilenet_v2\", pretrained=False\n",
    "        )\n",
    "        print(\"self.base\", self.base)\n",
    "        self.base.classifier = nn.Sequential(\n",
    "            nn.LazyLinear(512), nn.Dropout(), nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        return x\n",
    " \n",
    " \n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    hyperparameters: dict,\n",
    "    args: object,\n",
    "    device: str,\n",
    "):\n",
    "    \"\"\"Train model\n",
    " \n",
    "    Args:\n",
    "        model (nn.Module): Model to train\n",
    "        train_dataloader (DataLoader): Dataloader to use for training\n",
    "        test_dataloader (DataLoader): Dataloader to use for testing\n",
    "        hyperparameters (dict): Dictionary with hyperparameters\n",
    "        device (str): Device to use when training the model\n",
    "    \"\"\"\n",
    "    # Create optimizer and loss function\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=hyperparameters[\"lr\"])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=hyperparameters[\"scheduler_step_size\"],\n",
    "        gamma=hyperparameters[\"scheduler_gamma\"],\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "    for epoch in range(hyperparameters[\"epochs\"]):\n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    " \n",
    "        # Create tqdm progress bar\n",
    "        with tqdm.tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch}\") as pbar:\n",
    "            for (x, y) in train_dataloader:\n",
    "                # Move batch to device\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    " \n",
    "                # Forward pass\n",
    "                y_hat = model(x)\n",
    " \n",
    "                # Compute loss\n",
    "                loss = criterion(y_hat, y)\n",
    " \n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    " \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    " \n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    " \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    " \n",
    "                # Log loss to wandb\n",
    "                #wandb.log({\"loss\": loss.item(), \"lr\": scheduler.get_last_lr()[0]})\n",
    " \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    " \n",
    "        # Evaluate model\n",
    "        test(model, test_dataloader, device)\n",
    " \n",
    "        # Save model, scheduler and optimizer checkpoint\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            os.path.join(args.out_dir, \"last.pt\"),\n",
    "        )\n",
    " \n",
    " \n",
    "def test(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    final_eval: bool = False,\n",
    "):\n",
    "    \"\"\"Test model by plotting the embeddings in 2D space\n",
    " \n",
    "    Args:\n",
    "        model (nn.Module): Model to test\n",
    "        dataloader (DataLoader): Dataloader to use for testing\n",
    "    \"\"\"\n",
    "    # Create Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    " \n",
    "    logs_dict = {}\n",
    " \n",
    "    losses = []\n",
    " \n",
    "    # List to store predictions and labels\n",
    "    predictions = []\n",
    "    labels = []\n",
    " \n",
    "    # Disable gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in dataloader:\n",
    "            # Move batch to device\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    " \n",
    "            # Forward pass\n",
    "            output = model(x)\n",
    "            prediction = torch.argmax(output, dim=1)\n",
    " \n",
    "            losses.append(criterion(output, y).item())\n",
    "            predictions.append(prediction.cpu().numpy())\n",
    "            labels.append(y.cpu().numpy())\n",
    " \n",
    "        # Concatenate embeddings into a np array of shape: (num_samples, embedding_dim)\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    " \n",
    "        # Compute loss\n",
    "        loss = np.mean(losses)\n",
    "        logs_dict[\"Test Loss\"] = loss\n",
    " \n",
    "        # Compute accuracy\n",
    "        accuracy = (predictions == labels).sum() / labels.shape[0]\n",
    "        logs_dict[\"Test Accuracy\"] = accuracy\n",
    " \n",
    "        if final_eval:\n",
    "            # Compute Confusion Matrix\n",
    "            cm = confusion_matrix(labels, predictions)\n",
    "            cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "            cm_display.figure_.savefig(\n",
    "                os.path.join(args.out_dir, \"confusion_matrix.png\")\n",
    "            )\n",
    "            logs_dict[\"Confusion Matrix\"] = wandb.Image(\n",
    "                os.path.join(args.out_dir, \"confusion_matrix.png\")\n",
    "            )\n",
    " \n",
    "        # Log to wandb\n",
    "        #wandb.log(logs_dict)\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    # Define args\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Example of good practices in an ML script\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hyperparameters\",\n",
    "        default=\"example.yml\",\n",
    "        type=str,\n",
    "        help=\"Path to yaml file with hyperparameters\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_dir\",\n",
    "        default=\"./output\",\n",
    "        type=str,\n",
    "        help=\"Path to the output directory where the model will be saved\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        default=\"/data\",\n",
    "        type=str,\n",
    "        help=\"Path to the data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--preload\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to preload the data in memory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prefetch_factor\",\n",
    "        default=2,\n",
    "        type=int,\n",
    "        help=\"Number of batches loaded in advance by each worker.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--experiment_name\",\n",
    "        default=\"example\",\n",
    "        type=str,\n",
    "        help=\"Name to identify the experiment\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        default=\"cuda:0\",\n",
    "        type=str,\n",
    "        help=\"Device to use when training the model\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    with open(args.hyperparameters, \"r\") as f:\n",
    "        hyperparameters = yaml.safe_load(f)\n",
    " \n",
    "    # Create output directory\n",
    "    args.out_dir = os.path.join(args.out_dir, args.experiment_name)\n",
    "    if os.path.isdir(args.out_dir):\n",
    "        args.experiment_name = (\n",
    "            args.experiment_name + \"_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        )\n",
    "        args.out_dir = os.path.join(\n",
    "            args.out_dir,\n",
    "            args.experiment_name,\n",
    "        )\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    " \n",
    "    # Set device\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "    # Set seed\n",
    "    torch.manual_seed(hyperparameters[\"seed\"])\n",
    "    np.random.seed(hyperparameters[\"seed\"])\n",
    " \n",
    "    # Create model\n",
    "    model = Model()\n",
    " \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    " \n",
    "    # Create data augmentations\n",
    "    data_transform = T.Compose(\n",
    "        [   \n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    )\n",
    "    file_path = \"clinical_data.json\"\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "         data = json.load(json_file)\n",
    "    print(len(data))\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for i in data:\n",
    "        image_paths = image_paths +  [\"data/main_branch/ideal/images/\" + i[\"filename\"]]\n",
    "        if i[\"ifr\"] <0.89:\n",
    "            labels = labels + [1]\n",
    "        else:\n",
    "            labels = labels + [0]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = MNISTDataset(image_paths,labels, data_transform)\n",
    "\n",
    " \n",
    "    test_dataset = MNISTDataset(image_paths,labels, data_transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=hyperparameters[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        prefetch_factor=args.prefetch_factor,\n",
    "    )\n",
    "    \"\"\"\n",
    "    for images, labels in train_dataloader:\n",
    "        print(images)\n",
    "        image_testing = images[0]\n",
    "        label_testing = labels[0]\n",
    "        break\n",
    "    image_testing.to(device)\n",
    "    label_testing.to(device)\n",
    "    model.eval()\n",
    "    print(torch.argmax(model(image_testing)))\n",
    "    #print(train_dataset.__getitem__(0))\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "\n",
    "    for inputs, _ in data_loader:\n",
    "        mean += inputs.mean((0, 2, 3))\n",
    "        std += inputs.std((0, 2, 3))\n",
    "\n",
    "    mean /= len(dataset)\n",
    "    std /= len(dataset)\n",
    "\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Std:\", std)\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(train_dataset.__getitem__(0))\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=hyperparameters[\"batch_size\"],\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=args.prefetch_factor,\n",
    "    )\n",
    "\n",
    "    # Create Logger\n",
    "    \"\"\"\n",
    "    wandb.init(\n",
    "        project=\"example\",\n",
    "        name=args.experiment_name,\n",
    "        config=hyperparameters,\n",
    "        save_code=True,\n",
    "    )\n",
    "    \"\"\"\n",
    "    #Train model\n",
    "    #train(model, train_dataloader, test_dataloader, hyperparameters, args, device)\n",
    "    #Test model\n",
    "    #test(model, test_dataloader, device, final_eval=True)\n",
    "  \n",
    "# Print the model output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
